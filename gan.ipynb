{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiswind/machine-learning-study/blob/main/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTkzzNKQaBRb",
        "outputId": "5c5c9034-169c-4799-8c86-6d6e7a3e841c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  9 16:26:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ln -s drive/MyDrive/colab $(pwd)/colab\n",
        "!mkdir -p ./colab/gan\n",
        "DRIVE_DIR = './colab/gan/'\n",
        "!ls -l\n",
        "!ls -l ./colab/gan"
      ],
      "metadata": {
        "id": "OrNos3c7eO-0",
        "outputId": "ad7477ad-f275-4be4-a930-5690d7906c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/colab/colab': File exists\n",
            "total 1668632\n",
            "lrwxrwxrwx 1 root root        19 Nov  9 15:40 colab -> drive/MyDrive/colab\n",
            "-rw-r--r-- 1 root root 853083403 Nov  9 15:56 dogs-vs-cats-redux-kernels-edition.zip\n",
            "drwx------ 5 root root      4096 Nov  9 15:40 drive\n",
            "drwxr-xr-x 1 root root      4096 Nov  7 14:37 sample_data\n",
            "-rw-r--r-- 1 root root    113903 Dec 11  2019 sample_submission.csv\n",
            "drwxr-xr-x 2 root root    286720 Sep 20  2013 test\n",
            "-rw-r--r-- 1 root root 284478493 Dec 11  2019 test.zip\n",
            "drwxr-xr-x 2 root root    770048 Sep 20  2013 train\n",
            "-rw-r--r-- 1 root root 569918665 Dec 11  2019 train.zip\n",
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"windthis\",\"key\":\"a968dadd8de108941769a83c92588874\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!rm -f *.zip\n",
        "!rm -f *.csv\n",
        "!rm -rf ./train/\n",
        "!rm -rf ./test/\n",
        "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition\n",
        "!unzip -q dogs-vs-cats-redux-kernels-edition.zip\n",
        "!unzip -q ./train.zip\n",
        "!unzip -q ./test.zip"
      ],
      "metadata": {
        "id": "zg2JyzT8gzlp",
        "outputId": "a40472b7-d875-42c7-9b1f-5a0dd29d3f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dogs-vs-cats-redux-kernels-edition.zip to /content\n",
            "100% 813M/814M [00:22<00:00, 42.1MB/s]\n",
            "100% 814M/814M [00:22<00:00, 38.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入开发需要的库\n",
        "import keras\n",
        "import os\n",
        "import shutil\n",
        "import threading\n",
        "import numpy as np\n",
        "import cv2\n",
        "import h5py\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "from keras.applications import *\n",
        "from keras.preprocessing import *\n",
        "from keras.preprocessing.image import *\n"
      ],
      "metadata": {
        "id": "gPW3YrR38bad"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(load_type=\"train\"):\n",
        "    path = None\n",
        "    n = 25000\n",
        "    if load_type==\"train\":\n",
        "        imgs = []\n",
        "        labels = []\n",
        "        \n",
        "        path = \"./train/\"\n",
        "        img_names = os.listdir(path)\n",
        "        \n",
        "        for name in img_names:\n",
        "            imgs.append(\"train/\"+name)\n",
        "            labels.append([0] if name[:3] == \"cat\" else [1])\n",
        "            \n",
        "        train_img_names,valid_img_names,train_labels,valid_labels = train_test_split(imgs,labels,test_size=0.2,random_state=42)\n",
        "        return train_img_names,valid_img_names,train_labels,valid_labels\n",
        "    else:\n",
        "        # test,don`t have the labels\n",
        "        path = \"./test\"\n",
        "        img_names = os.listdir(path)\n",
        "        imgs = []\n",
        "        for img in img_names:\n",
        "            imgs.append(img)\n",
        "                \n",
        "        return imgs\n"
      ],
      "metadata": {
        "id": "kKbMVGf0-UHw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_names,valid_img_names,train_labels,valid_labels = load_data()"
      ],
      "metadata": {
        "id": "0lnH_CKbO0yR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_img_names[:5])\n",
        "print(train_labels[:5])\n",
        "print(len(train_img_names)+len(valid_img_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEvCCUxO3VY",
        "outputId": "b6f3f6d7-bac4-4063-9360-844b3719ace2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train/dog.2474.jpg', 'train/cat.11618.jpg', 'train/cat.12202.jpg', 'train/dog.4031.jpg', 'train/cat.5840.jpg']\n",
            "[[1], [0], [0], [1], [0]]\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义keras的generator方法\n",
        "# custom keras generator\n",
        "class MOGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, data, n, des_size=(224, 224), means=None, stds=None,\n",
        "                 is_directory=True, batch_size=32, shuffle=True, seed=0):\n",
        "        '''\n",
        "        data: tuple of (x,y)\n",
        "        n: data size\n",
        "        des_size: standard size\n",
        "        means: the dataset mean of RGB,default is imagenet means [103.939, 116.779, 123.68]\n",
        "        batch_size: default is 32\n",
        "        shuffle: random the data,default is True\n",
        "        '''\n",
        "        self.x = np.array(data[0])\n",
        "        if len(data) >= 2:\n",
        "            self.y = np.array(data[1])\n",
        "        else:\n",
        "            self.y = None\n",
        "        self.n = n\n",
        "        self.des_size = des_size\n",
        "        self.is_directory = is_directory\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.lock = threading.Lock()\n",
        "        self.index_array = self._set_index_array()\n",
        "        self.means = means\n",
        "        self.stds = stds\n",
        "\n",
        "    def reset_index(self):\n",
        "        self.batch_index = 0\n",
        "\n",
        "    def _set_index_array(self):\n",
        "        self.index_array = np.arange(self.n)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.index_array)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # 重置索引数组\n",
        "        self._set_index_array()\n",
        "\n",
        "    def __len__(self):\n",
        "        # 计算batch的总数量\n",
        "        return int(np.ceil(self.n / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.index_array is None:\n",
        "            self._set_index_array()\n",
        "        index_array = self.index_array[self.batch_size * idx:\n",
        "                                       self.batch_size * (idx + 1)]\n",
        "        return self._data_generate(index_array)\n",
        "\n",
        "    def _data_generate(self, index_array):\n",
        "        # read from path\n",
        "        # request the memory\n",
        "        imgs = np.zeros((len(index_array), self.des_size[0], self.des_size[1], 3), dtype=np.uint8)\n",
        "        # read the data\n",
        "        if self.is_directory:\n",
        "            img_names = self.x[index_array]\n",
        "            for name_index in range(len(img_names)): \n",
        "                img = cv2.imread(img_names[name_index])\n",
        "                if img is not None:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    img = cv2.resize(img, self.des_size)\n",
        "                    imgs[name_index] = img\n",
        "        else:\n",
        "            for i in range(len(index_array)):\n",
        "                img = self.x[index_array[i]]\n",
        "                img = cv2.resize(img, self.des_size)\n",
        "                imgs[i] = img\n",
        "        if self.y is not None:\n",
        "            labels = self.y[index_array]\n",
        "        if labels is None:\n",
        "            return imgs\n",
        "        else:\n",
        "            return imgs, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "O2Wg2GxqO5YT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_img_names,valid_img_names,train_labels,valid_labels = load_data()\n",
        "\n",
        "test_img_names = load_data(load_type=\"test\")\n",
        "\n",
        "train_generator_224 = MOGenerator((train_img_names,train_labels), len(train_img_names), des_size=(224,224),\n",
        "                                  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_generator_299 = MOGenerator((train_img_names,train_labels), len(train_img_names), des_size=(299,299),\n",
        "                                  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_generator_299 = MOGenerator((valid_img_names,valid_labels), len(valid_img_names), des_size=(299,299),\n",
        "                                  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "valid_generator_224 = MOGenerator((valid_img_names,valid_labels), len(valid_img_names), des_size=(224,224),\n",
        "                                  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_generator_299 = MOGenerator((test_img_names), len(test_img_names), des_size=(299,299),\n",
        "                                  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_generator_224 = MOGenerator((test_img_names), len(test_img_names), des_size=(224,224),\n",
        "                                  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "o7MOSNxjO-2F"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications import resnet50"
      ],
      "metadata": {
        "id": "M9lcEStCPDRA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载Resnet50网络, Include_top: 是否包含卷积之后的全连接层.\n",
        "base_model = ResNet50(input_tensor=Lambda(resnet50.preprocess_input)(Input(shape=(224,224,3))), \n",
        "                      weights=\"imagenet\", include_top=False)\n",
        "# 遍历所有层, 将预训练模型的卷积层设置为不可训练, 这样它们的参数就不会被改变了.\n",
        "for layers in base_model.layers:\n",
        "    layers.trainable = False\n",
        "\n",
        "# 重新设置输出层, 我们的类别是两类, 只需要一个神经元即可.\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# 实例化模型\n",
        "model = Model(base_model.input, x)\n",
        "\n",
        "# 设置优化器, 损失函数, 展示参数信息\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# 开始训练, 通过设置迭代器模式\n",
        "model.fit_generator(train_generator_224,len(train_img_names)//batch_size,epochs=5,\n",
        "                    validation_data=valid_generator_224,validation_steps=len(valid_img_names)//batch_size,shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUkZiu57PRy3",
        "outputId": "e1eaf25e-2995-479d-e739-71a67d36955e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 90s 140ms/step - loss: 0.0540 - accuracy: 0.9812 - val_loss: 0.0369 - val_accuracy: 0.9860\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 94s 151ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: 0.0348 - val_accuracy: 0.9882\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 95s 152ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 0.0383 - val_accuracy: 0.9878\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 85s 136ms/step - loss: 0.0254 - accuracy: 0.9909 - val_loss: 0.0376 - val_accuracy: 0.9878\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 86s 138ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 0.0343 - val_accuracy: 0.9880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e80371c50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inception\n",
        "inception = inception_v3.InceptionV3(include_top=False,\n",
        "        weights=\"imagenet\",input_tensor=Lambda(inception_v3.preprocess_input)(Input(shape=(299,299,3))),pooling=\"avg\")\n",
        "output = inception.output\n",
        "\n",
        "output = Dropout(0.25)(output)\n",
        "\n",
        "prediction = Dense(1,activation=\"sigmoid\")(output)\n",
        "\n",
        "inception_model = Model(inputs=inception.input,outputs=prediction)\n",
        "\n",
        "for layer in inception.layers: #[:-res_global_pool_index]\n",
        "    layer.trainable = False\n",
        "\n",
        "inception_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "inception_model.fit_generator(train_generator_299,(len(train_img_names) + batch_size - 1)//batch_size,epochs=5,\n",
        "                       validation_data=valid_generator_299,validation_steps=len(valid_img_names)//batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgS4o6wxPuXA",
        "outputId": "2c3cb0cc-2dc4-4425-e646-69f6f79962d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/625 [======>.......................] - ETA: 58s - loss: 0.1372 - accuracy: 0.9640"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## xception\n",
        "xcep = Xception(include_top=False, weights=\"imagenet\", \n",
        "                input_tensor=Lambda(xception.preprocess_input)(Input(shape=(299,299,3))),pooling=\"avg\")\n",
        "output = xcep.output\n",
        "\n",
        "output = Dropout(0.25)(output)\n",
        "\n",
        "prediction = Dense(1,activation=\"sigmoid\")(output)\n",
        "\n",
        "xcep_model = Model(inputs=xcep.input,outputs=prediction)\n",
        "\n",
        "for layer in xcep.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "xcep_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "xcep_model.fit_generator(train_generator_299,(len(train_img_names) + batch_size - 1)//batch_size,epochs=5,\n",
        "                       validation_data=valid_generator_299,validation_steps=len(valid_img_names)//batch_size)\n"
      ],
      "metadata": {
        "id": "Sjm7Ta1vRESl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"resnet.h5\")\n",
        "model.save_weights(\"xcep.h5\")\n",
        "model.save_weights(\"incep.h5\")\n"
      ],
      "metadata": {
        "id": "bie7Iij4RHoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import *\n",
        "\n",
        "# 获取所有训练集的图片名\n",
        "train_filenames = os.listdir(\"train\")\n",
        "test_filenames = os.listdir(\"test\")\n",
        "\n",
        "# 文件名是特殊命名形式的, 所以直接获取分类后的文件\n",
        "train_cat = list(filter(lambda x:x[:3] == 'cat', train_filenames))\n",
        "train_dog = list(filter(lambda x:x[:3] == 'dog', train_filenames))\n",
        "def rmrf_mkdir(dirname):\n",
        "    if os.path.exists(dirname):\n",
        "        shutil.rmtree(dirname)\n",
        "    os.mkdir(dirname)\n",
        "\n",
        "# 创建目录\n",
        "rmrf_mkdir('train2')\n",
        "os.mkdir('train2/cat')\n",
        "os.mkdir('train2/dog')\n",
        "\n",
        "rmrf_mkdir('valid2')\n",
        "os.mkdir('valid2/cat')\n",
        "os.mkdir('valid2/dog')\n",
        "\n",
        "rmrf_mkdir('test2')\n",
        "os.mkdir(\"test2/test\")\n",
        "for filename in test_filenames:\n",
        "  # 建立软链（不会从创建文件, 有点类似快捷方式）\n",
        "  os.symlink(\"/content/test/\"+filename, \"/content/test2/test/\"+filename)\n",
        "\n",
        "for filename in train_cat[:-2500]:\n",
        "  os.symlink(\"/content/train/\"+filename, \"/content/train2/cat/\"+filename)\n",
        "\n",
        "for filename in train_dog[:-2500]:\n",
        "  os.symlink(\"/content/train/\"+filename, \"/content/train2/dog/\"+filename)\n",
        "\n",
        "for filename in train_cat[-2500:]:\n",
        "  os.symlink(\"/content/train/\"+filename, \"/content/valid2/cat/\"+filename)\n",
        "    \n",
        "for filename in train_dog[-2500:]:\n",
        "  os.symlink(\"/content/train/\"+filename, \"/content/valid2/dog/\"+filename)\n",
        "    \n",
        "# 使用Keras默认的生成器, 因为我们已经生成对应的目录了\n",
        "# 224 是 resnet的处理方式, 299是xception, inception的处理大小\n",
        "gen = ImageDataGenerator()\n",
        "\n",
        "train_generator_224 = gen.flow_from_directory(\"train2\", (224,224), shuffle=False, \n",
        "                                              batch_size=batch_size,class_mode='binary')\n",
        "\n",
        "valid_generator_224 = gen.flow_from_directory(\"valid2\", (224,224), shuffle=False, \n",
        "                                              batch_size=batch_size,class_mode='binary')\n",
        "\n",
        "test_generator_224 = gen.flow_from_directory(\"test2\", (224,224), shuffle=False, \n",
        "                                             batch_size=batch_size, class_mode=None)\n",
        "\n",
        "train_generator_299 = gen.flow_from_directory(\"train2\", (299,299), shuffle=False, \n",
        "                                              batch_size=batch_size,class_mode='binary')\n",
        "\n",
        "valid_generator_299 = gen.flow_from_directory(\"valid2\", (299,299), shuffle=False, \n",
        "                                              batch_size=batch_size,class_mode='binary')\n",
        "\n",
        "test_generator_299 = gen.flow_from_directory(\"test2\", (299,299), shuffle=False, \n",
        "                                             batch_size=batch_size, class_mode=None)\n"
      ],
      "metadata": {
        "id": "bUBIX544RK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# 特征向量的保存方法\n",
        "def write_feature(model_name,model,train_generator,train_labels,valid_generator,valid_labels,test_generator,batch_size=32):\n",
        "    # 通过模型名字来加载不同的权重\n",
        "    if model_name == 'resnet_feature':\n",
        "        model.load_weights('resnet.h5',by_name=True)\n",
        "    elif model_name == 'inception_feature':\n",
        "        model.load_weights('incep.h5',by_name=True)\n",
        "    else:\n",
        "        model.load_weights('xcep.h5',by_name=True)\n",
        "    # 转换为numpy数组\n",
        "    train_labels = np.array(train_labels)\n",
        "    valid_labels = np.array(valid_labels)\n",
        "    # 直接进行输出, 得到特征向量\n",
        "    train_feature = model.predict_generator(train_generator,int(np.ceil(train_generator.samples/batch_size)),verbose=1)\n",
        "    valid_feature = model.predict_generator(valid_generator,int(np.ceil(valid_generator.samples/batch_size)),verbose=1)\n",
        "    test_feature  = model.predict_generator(test_generator,int(np.ceil(test_generator.samples/batch_size)),verbose=1)\n",
        "    print(\"train_feature.shape:\",train_feature.shape)\n",
        "    print(\"valid_feature.shape:\",valid_feature.shape)\n",
        "    # 保存到本地\n",
        "    with h5py.File(model_name+'.h5','w') as file:\n",
        "        file.create_dataset(\"train\",data=train_feature,dtype=\"float32\")\n",
        "        file.create_dataset('trian_labels',data=np.array(train_generator.classes),dtype=\"uint8\")\n",
        "        file.create_dataset(\"valid\",data=valid_feature,dtype=\"float32\")\n",
        "        file.create_dataset(\"valid_labels\",data=np.array(valid_generator.classes),dtype=\"uint8\")\n",
        "        file.create_dataset(\"test\",data=test_feature,dtype=\"float32\")\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "jcTJqUZmRNwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet50\n",
        "write_feature('resnet_feature',Model(inputs=model.input,outputs=model.layers[-3].output),\n",
        "              train_generator_224,train_labels,valid_generator_224,valid_labels,test_generator_224)\n"
      ],
      "metadata": {
        "id": "bw_jkJoPRR4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inception\n",
        "write_feature('inception_feature',Model(inputs=inception_model.input,outputs=inception_model.layers[-3].output),\n",
        "              train_generator_299,train_labels, valid_generator_299,valid_labels,test_generator_299)\n",
        "\n"
      ],
      "metadata": {
        "id": "s5BcrvgPRVpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xception\n",
        "write_feature('xception_feature',Model(inputs=xcep_model.input,outputs=xcep_model.layers[-3].output),\n",
        "              train_generator_299,train_labels,valid_generator_299,valid_labels,test_generator_299)\n"
      ],
      "metadata": {
        "id": "ezaMm3wzRYjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_files = ['resnet_feature.h5','inception_feature.h5','xception_feature.h5']\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_valid = []\n",
        "y_valid = []\n",
        "X_test = []\n",
        "\n",
        "for file_name in feature_files:\n",
        "    with h5py.File(file_name, 'r') as h:\n",
        "        X_train.append(np.array(h['train']))\n",
        "        X_valid.append(np.array(h['valid']))\n",
        "        X_test.append(np.array(h['test']))\n",
        "        y_train = np.array(h['trian_labels'])\n",
        "        y_valid = np.array(h['valid_labels'])\n",
        "        print(np.array(h['train']).shape,np.array(h['valid']).shape,np.array(h['test']).shape)\n",
        "\n",
        "X_train = np.concatenate(X_train, axis=1)\n",
        "X_valid = np.concatenate(X_valid, axis=1)\n",
        "X_test = np.concatenate(X_test, axis=1)\n",
        "\n",
        "print(\"last:\",X_train.shape,X_valid.shape,X_test.shape)\n"
      ],
      "metadata": {
        "id": "WKQB-Am1RbbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "# 打乱数据，模拟现实中的随机效果\n",
        "X_train, y_train = shuffle(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "6zuZuyi0RegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.utils\n",
        "\n",
        "input_tensor = Input(X_train.shape[1:])\n",
        "x = input_tensor\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "concatenate_model = Model(inputs=input_tensor, outputs=x)\n",
        "\n",
        "concatenate_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "concatenate_model.fit(X_train,y_train,batch_size=128, epochs=5,validation_data=(X_valid,y_valid))#validation_split=0.2\n"
      ],
      "metadata": {
        "id": "oD-tl4VjRg29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "y_pred = concatenate_model.predict(X_test, verbose=1)\n",
        "y_pred = y_pred.clip(min=0.005, max=0.995)\n",
        "\n",
        "df = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "image_size = (224, 224)\n",
        "gen = ImageDataGenerator()\n",
        "test_generator = gen.flow_from_directory(\"/content/test2/\", image_size, shuffle=False, \n",
        "                                         batch_size=16, class_mode=None)\n",
        "\n",
        "for i, fname in enumerate(test_generator.filenames):\n",
        "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
        "    df.set_value(index-1, 'label', y_pred[i])\n",
        "\n",
        "df.to_csv('sample_submission.csv', index=None)\n",
        "\n",
        "print(df.head(20))\n"
      ],
      "metadata": {
        "id": "hhJgRdr_Rjck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c dogs-vs-cats-redux-kernels-edition -f sample_submission.csv -m \"Message\"\n"
      ],
      "metadata": {
        "id": "xo9ZZdOLRnPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSAh-F3sRp69"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}